class PositionalEmbedings(nn.Module):
  def __init__(self, max_seq_len, emb_size, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.max_seq_len = max_seq_len
    self.emb_size = emb_size
    self.matrix = torch.nn.Embedding(max_seq_len, emb_size)

  def forward(self, seq_len):
    '''
    x - batch_size * seq_len
    '''
    return self.matrix.weight[0:seq_len]