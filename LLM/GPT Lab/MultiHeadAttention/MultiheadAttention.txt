class MultiHeadAttention(nn.Module):
  def __init__(self, num_heads: int, emb_size: int,head_size: int, max_seq_len: int, dropout = 0.1) -> None:
    '''
    num_heads (int) - Количество голов
    emb_size (тип int) - размерность эмбедингов (позиционных токнеов)
    head_size (int) - размерность W-матриц (W_k, W_q, W_v)
    max_head_len (int) - максимально возможная длина последовательности
    dropout (float, от 0 до 1) - вероятность обнулить значения тензора в слое dropout
    '''
    super().__init__()
    self.heads = [HeadAttention(emb_size, head_size, max_seq_len) for i in range(num_heads) ]
    self.l1 = torch.nn.Linear(head_size * num_heads, emb_size)
    self.dr = nn.Dropout(p=dropout)

    self.emb_size = emb_size
    self.head_size = head_size
    self.max_seq_len = max_seq_len
    self.num_heads = num_heads
    self.dropout = dropout

  def forward(self, x):
    res = []
    for head in self.heads:
        res.append(head.forward(x))

    res_tensor = torch.cat(res, dim=2)
    return self.dr(self.l1(res_tensor))
